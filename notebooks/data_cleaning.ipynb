{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QRT ENS Challenge Data 2023 - Benchmark\n",
    "\n",
    "Ce notebook détaille la construction du benchmark de ce challenge - il peut également être utile aux participants pour se lancer dans la compétition. \n",
    "\n",
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données\n",
    "\n",
    "- `X_train` et `X_test` ont  $10$ colonnes qui représentent les même variables explicatives mais sur des périodes de temps différentes il y a au plus $10 604$ lignes. \n",
    "\n",
    "- `X_train` et `y_train` partagent la même colonne `DELIVERY_START` - chaque ligne a un DELIVERY_START unique associéz à une date et heure de livraison de l'électricité\n",
    ". \n",
    "\n",
    "- La variable cible `spot_id_delta` de `y_train` correspond à l'écart entre le VWAP des transactions sur le marché infra-journalier (Intraday) et le prix SPOT pour 1MWh d'électricité (spot_id_delta = Intraday - SPOT) : si la valeur est positive, le prix Intraday est supérieur au prix SPOT et inversement.\n",
    "\n",
    "- **On notera que certaines colonnes ont des valeurs manquantes**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After downloading the X_train/X_test/Y_train .csv files in your working directory:\n",
    "X = pd.read_csv('../raw_data/X_train_Wwou3IE.csv')\n",
    "y = pd.read_csv('../raw_data/y_train_jJtXgMX.csv')\n",
    "X_rendu = pd.read_csv('../raw_data/X_test_GgyECq8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_seconds(hour_stamp):\n",
    "    return int(hour_stamp.split(\":\")[0])*60*60\n",
    "\n",
    "def date_stamp(df):\n",
    "    df[\"year\"] = df[\"DELIVERY_START\"].apply(lambda x: x.split(\"-\")[0])\n",
    "    df[\"month\"] = df[\"DELIVERY_START\"].apply(lambda x: x.split(\"-\")[1])\n",
    "    df[\"day\"] = df[\"DELIVERY_START\"].apply(lambda x: (x.split(\"-\")[2]).split(\" \")[0])\n",
    "    df[\"seconds\"] = df[\"DELIVERY_START\"].apply(lambda x: to_seconds(x.split(\" \")[1]))\n",
    "    return df\n",
    "    \n",
    "X = date_stamp(X)\n",
    "X_rendu  = date_stamp(X_rendu)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On manipule les données pour enlever les NaN qui ne nous intéressent pas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_kept = X_train.dropna(subset=['coal_power_available', 'gas_power_available', 'nucelear_power_available', 'wind_power_forecasts_average', 'solar_power_forecasts_average', 'wind_power_forecasts_std', 'solar_power_forecasts_std'])\n",
    "X_rendu_kept = X_train.dropna(subset=['coal_power_available', 'gas_power_available', 'nucelear_power_available', 'wind_power_forecasts_average', 'solar_power_forecasts_average', 'wind_power_forecasts_std', 'solar_power_forecasts_std'])\n",
    "\n",
    "marked_train = pd.concat([X_train, X_train_kept]).drop_duplicates(keep=False)['DELIVERY_START']\n",
    "marked_rendu = pd.concat([X_rendu, X_rendu_kept]).drop_duplicates(keep=False)['DELIVERY_START']\n",
    "\n",
    "X_train = X_train[~X_train['DELIVERY_START'].isin(marked_train)]\n",
    "y_train = y_train[~y_train['DELIVERY_START'].isin(marked_train)]\n",
    "X_rendu = X_rendu[~X_rendu['DELIVERY_START'].isin(marked_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '../data/y_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7bddf0b16530>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/y_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3385\u001b[0m         )\n\u001b[0;32m   3386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3387\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '../data/y_train.csv'"
     ]
    }
   ],
   "source": [
    "y_train.to_csv('../data/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "\n",
    "Représentation graphique des données de test de notre modèle :\n",
    "\n",
    "## 1) Histogramme des données d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Répartition des *spot_id_delta* dans *y_train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.distplot(y_train['spot_id_delta'], color='g', bins=1000, hist_kws={'alpha': 0.4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Corrélation générale entre les paramètres du modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(X_train, y_train, on='DELIVERY_START', how='inner')\n",
    "tab=df.corr()\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolor(tab, cmap='RdBu')\n",
    "row_labels = tab.columns\n",
    "col_labels = tab.index\n",
    "ax.set_xticks(np.arange(tab.shape[1])+0.5, minor= False)\n",
    "ax.set_yticks(np.arange(tab.shape[0])+0.5, minor= False)\n",
    "ax.set_xticklabels(row_labels, minor = False)\n",
    "ax.set_yticklabels(col_labels, minor = False)\n",
    "plt.xticks(rotation=90)\n",
    "fig.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "On s'intéresse ici à la présence de nombreuses valeurs NaN. Ces valeurs représentent un vrai manque pour l'entraînement de notre modèle car elles sont majoritairement situées sur les données très corrélées avec le *spot_id_delta*.\n",
    "\n",
    "## 1) Valeurs aberrantes dans *y_train*\n",
    "\n",
    "Comme vu lors de la partie Data Analysis, certaines des valeurs de *y_train* (3) ont des valeurs très éloignées des autres. On s'occupe donc ici de retirer ces valeurs qui ne semblent pas cohérentes pour notre modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 600\n",
    "\n",
    "eliminated_starts = y_train[abs(y_train['spot_id_delta']) - threshold >= 0].DELIVERY_START\n",
    "\n",
    "y_train = y_train[~y_train['DELIVERY_START'].isin(eliminated_starts)] # on ne sélectionne que les valeurs qui ne correspondent pas aux dates enlevées\n",
    "X_train = X_train[~X_train['DELIVERY_START'].isin(eliminated_starts)] # de même ici pour être cohérent sur le nombre de lignes\n",
    "\n",
    "print(len(y_train))\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminated_starts.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Prédiction de *load_forecast*\n",
    "\n",
    "Il y a environ 1500 données manquantes, que l'on va prédire à l'aide d'un modèle de régression Ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(X_train.isna(),cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_rendu.isna(),cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_spot_price_train = X_train['predicted_spot_price']\n",
    "predicted_spot_price_rendu = X_rendu['predicted_spot_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "y_load = X_train.dropna(subset=['load_forecast'])['load_forecast']\n",
    "X_load = X_train.dropna(subset=['load_forecast'])\n",
    "X_load = X_load.loc[:, X_load.columns != 'predicted_spot_price']\n",
    "X_load = X_load.loc[:, X_load.columns != 'load_forecast']\n",
    "\n",
    "X_train_load, X_test_load, y_train_load, y_test_load = train_test_split(X_load, y_load, test_size=0.3)\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train_load.set_index('DELIVERY_START'), y_train_load)\n",
    "print('linear regression score: ', clf.score(X_test_load.set_index('DELIVERY_START'), y_test_load))\n",
    "\n",
    "clf = GradientBoostingRegressor()\n",
    "clf.fit(X_train_load.set_index('DELIVERY_START'), y_train_load)\n",
    "print('gradient boosting regression score: ', clf.score(X_test_load.set_index('DELIVERY_START'), y_test_load))\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(X_train_load.set_index('DELIVERY_START'), y_train_load)\n",
    "print('random forest regression score: ', clf.score(X_test_load.set_index('DELIVERY_START'), y_test_load))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On choisit donc ici la régression ayant le meilleur score : Random Forest. On peut maintenant prédire les valeurs manquantes pour le *load_forecast* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict_load = X_train[X_train['load_forecast'].isna()]\n",
    "predicted_spot_price1 = X_predict_load['predicted_spot_price']\n",
    "load_forecast1 = X_predict_load['load_forecast']\n",
    "X_predict_load = X_predict_load.loc[:, ~(X_predict_load.columns.isin(['predicted_spot_price', 'load_forecast']))]\n",
    "\n",
    "X_predict_load.insert(1, 'load_forecast', clf.predict(X_predict_load.set_index('DELIVERY_START')))\n",
    "X_predict_load.insert(9, 'predicted_spot_price', predicted_spot_price1)\n",
    "\n",
    "#X_predict_load_rendu = X_rendu[X_rendu['load_forecast'].isna()]\n",
    "#predicted_spot_price2 = X_predict_load_rendu['predicted_spot_price']\n",
    "#load_forecast2 = X_predict_load_rendu['load_forecast']\n",
    "#X_predict_load_rendu = X_predict_load_rendu.loc[:, ~(X_predict_load_rendu.columns.isin(['predicted_spot_price', 'load_forecast']))]\n",
    "\n",
    "#X_predict_load_rendu.insert(1, 'load_forecast', clf.predict(X_predict_load_rendu.set_index('DELIVERY_START')))\n",
    "#X_predict_load_rendu.insert(9, 'predicted_spot_price', predicted_spot_price2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train[~X_train['load_forecast'].isna()], X_predict_load])\n",
    "#X_rendu = pd.concat([X_rendu[~X_rendu['load_forecast'].isna()], X_predict_load_rendu])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['predicted_spot_price'] = predicted_spot_price_train\n",
    "#X_rendu['predicted_spot_price'] = predicted_spot_price_rendu\n",
    "X_train['predicted_spot_price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Prédiction de *predicted_spot_price*\n",
    "\n",
    "Maintenant, les valeurs pour de *load_forecast* NaN ont été remplacées. De part la forte corrélation en *predicted_spot_price* en *load_forecast*, et comme on dispose de toutes les valeurs de *load_forecast*, on peut maintenant créer un modèle capable de nous prédire les valeurs de *predicted_spot_price* manquantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_train.isna(),cbar=False) # vérification qu'il n'y a plus de valeur manquante dans load_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "\n",
    "delivery_starts = X_train['DELIVERY_START']\n",
    "\n",
    "y_spot = X_train.dropna(subset=['predicted_spot_price'])['predicted_spot_price']\n",
    "X_spot = X_train.dropna(subset=['predicted_spot_price'])\n",
    "X_spot = X_spot.loc[:, X_spot.columns != 'predicted_spot_price']\n",
    "X_spot = X_spot.set_index('DELIVERY_START')\n",
    "\n",
    "print('nombre de valeurs non NaN: ', len(y_spot))\n",
    "\n",
    "X_train_spot, X_test_spot, y_train_spot, y_test_spot = train_test_split(X_spot, y_spot, test_size=0.25)\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(X_train_spot, y_train_spot)\n",
    "print('linear regression score: ', clf.score(X_test_spot, y_test_spot))\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(X_train_spot, y_train_spot)\n",
    "print('random forest regression score: ', clf.score(X_test_spot, y_test_spot))\n",
    "\n",
    "clf = GradientBoostingRegressor()\n",
    "clf.fit(X_train_spot, y_train_spot)\n",
    "print('gradient boosting regression score: ', clf.score(X_test_spot, y_test_spot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict_spot = X_train[X_train['predicted_spot_price'].isna()]\n",
    "X_predict_spot = X_predict_spot.loc[:, X_predict_spot.columns != 'predicted_spot_price']\n",
    "X_predict_spot = X_predict_spot.set_index('DELIVERY_START')\n",
    "\n",
    "X_predict_rendu = X_rendu[X_rendu['predicted_spot_price'].isna()]\n",
    "X_predict_rendu = X_predict_rendu.loc[:, X_predict_rendu.columns != 'predicted_spot_price']\n",
    "X_predict_rendu = X_predict_rendu.set_index('DELIVERY_START')\n",
    "\n",
    "X_predict_spot.insert(8, 'predicted_spot_price', clf.predict(X_predict_spot))\n",
    "X_predict_rendu.insert(8, 'predicted_spot_price', clf.predict(X_predict_rendu))\n",
    "X_predict_spot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train[~X_train['predicted_spot_price'].isna()], X_predict_spot.reset_index('DELIVERY_START')])\n",
    "X_rendu = pd.concat([X_rendu[~X_rendu['predicted_spot_price'].isna()], X_predict_rendu.reset_index('DELIVERY_START')])\n",
    "sns.heatmap(X_train.isna(),cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_rendu.isna(),cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('../data/X_train.csv')\n",
    "X_rendu.to_csv('../data/X_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque bien que le database est maintenant dépourvu de valeurs NaN !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astuces et idées d'amélioration\n",
    "\n",
    "- Réféchir à la modélation des différents facteurs qui font bouger les prix de l'électricité dans chaque pays pourra être utile. \n",
    "\n",
    "- Le jeu de données est relativement petit - c'est un \"small data challenge\" - alors attention à ne pas surapprendre les paramètres de vos modèles ! Il sera certainement utile de mettre en place de bonnes pratiques de validation croisée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm=LinearRegression()\n",
    "Z = X_train_clean\n",
    "lm.fit(Z, Y_train['spot_id_delta'])\n",
    "Y_hat = lm.predict(X_train_clean)\n",
    "sns.residplot(Z['load_forecast'], Y_train['spot_id_delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = [('Scale',StandardScaler()),('polynomial',PolynomialFeatures(degree=2)),('model',LinearRegression())]\n",
    "pipe=Pipeline(Input)\n",
    "pipe.fit(X_train_clean,Y_train['spot_id_delta'])\n",
    "Y_hat=pipe.predict(X_train_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit quand on trace les résidus de la regression linéaire entre le load_forecast en France et la différence, une distribution équiprobable partout ce qui laisse penser que la regression linéaire est correcte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSQ_test= []\n",
    "lr=LinearRegression()\n",
    "X_test_clean = X_test.drop(['DELIVERY_START'], axis=1).fillna(0)\n",
    "Y_test_clean = Y_train.drop(['DELIVERY_START'], axis=1).fillna(0)\n",
    "order = [1,2,3,4,5]\n",
    "for n in order :\n",
    "    pr = PolynomialFeatures(degree=n)\n",
    "    x_train_pr=pr.fit_transform(X_train_clean[['load_forecast']])\n",
    "    x_test_pr=pr.fit_transform(X_test_clean[['load_forecast']])\n",
    "    lr.fit(x_train_pr,Y_train_clean)\n",
    "    RSQ_test.append(lr.score(x_test_pr,Y_test_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "RidgeModel = Ridge(alpha=0.1)\n",
    "RidgeModel.fit(X_train_clean,Y_train_clean)\n",
    "Yhat=RidgeModel.predict(X_train_clean)\n",
    "\n",
    "parameters = [{'alpha': [0.001,2,10,100,1000,10000,100000,1000000000]}]\n",
    "RR=Ridge()\n",
    "Grid1=GridSearchCV(RR,parameters,cv=5)\n",
    "Grid1.fit(X_train_clean,Y_train_clean)\n",
    "Grid1.best_estimator_\n",
    "scores=Grid1.cv_results_\n",
    "\n",
    "\n",
    "for param, mean_val in zip(scores['params'],scores['mean_test_score']):\n",
    "    print(param, \"R^2 on test data:\", mean_val)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e98505b2ea4c5ad54dad79b106a9e9e74f288112ea588ce88c6ce949430e0824"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
